{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9110e4a6-9ee9-42ed-8914-5a8068d2147a",
   "metadata": {},
   "source": [
    "## CS 410 - FINAL PROJECT\n",
    "##### Thomas Downs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5476f98-e892-4c16-a8a1-6230c6f2ac2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### INTRO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b91c6-6db7-40bc-b864-5901a6913fc4",
   "metadata": {},
   "source": [
    "Feel free to run this from the top! It doesn't do anything too demanding, and all the default values for the dataframe and dictionary generation keep N <= 10000. The only required libraries are below, and if you have worked with Python in any capacity you likely have everything except for RapidFuzz already installed. If you don't want to install RapidFuzz, just know that the portions that use a string similarity metric will fail (SIMILAR QUERY TOKEN SUBSTITUTION and DOCUMENT RANKING NAME SIMILARITY TIEBREAKERS). However, if you have other Python libraries that have string similarity metrics, or have other string similarity metrics that you'd want to use, you should be able to replace JaroWinkler.similarity in the function calls and everything will work just as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f99f3-038d-4c7d-bea2-5e2dbc9c0c62",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a31ced-e08d-41c5-a73b-cd9a8b072c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil, floor, log\n",
    "from collections import defaultdict\n",
    "from rapidfuzz.distance import JaroWinkler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a792b6-4b6a-4a2c-b788-bacc3e8610b0",
   "metadata": {},
   "source": [
    "### NAME GENERATOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3980f5f-adaa-48b5-9a1d-8ad20cd67d41",
   "metadata": {},
   "source": [
    "The first goal of this project is to generate a baseline collection of names to use as a corpus. Due to not wanting to use any personally identifiable information (PII) I opted to create a basic mixture model to generate names, consisting of three unigram models. I found the 200 most common male and female first names, as well as the 400 most common (actually 397 because I copied it wrong) last names, both from government data for the year 2010. Then, I used the counts to create a basic probability function for each of the names and created a function generate n full names (first name + last name), approximately n/2 of which will be masculine and n/2 of which will be feminine.\n",
    "<br>\n",
    "<br>\n",
    "#### KEY ASSUMPTIONS \n",
    "- anglocentric names\n",
    "- the odds of any first name + last name are independent\n",
    "- no middle names, initials, hyphens, non-alphabetic characters, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e8796-2b6a-4d79-a37c-805f0d7922a0",
   "metadata": {},
   "source": [
    "sources:  \n",
    "https://www.ssa.gov/oact/babynames/decades/names2010s.html  \n",
    "https://www.census.gov/topics/population/genealogy/data/2010_surnames.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb24df92-01b8-4b6b-945f-76ddeadf175b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load CSVs\n",
    "data_types = {'name': 'string', 'count': 'int'}\n",
    "df_fem_nms = pd.read_csv(\"female_first_200.csv\", dtype=data_types)\n",
    "df_masc_nms = pd.read_csv(\"male_first_200.csv\", dtype=data_types)\n",
    "df_surnames = pd.read_csv(\"last_names_397.csv\", dtype=data_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4476a4a4-e3a9-48f4-baed-55b2e45c0248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data cleaning\n",
    "df_fem_nms.name = df_fem_nms.name.str.upper()\n",
    "df_masc_nms.name = df_masc_nms.name.str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fda45566-d592-4b0b-9ad6-8f33bac09c37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate unigram probabilities for all names\n",
    "df_fem_nms['prob'] = df_fem_nms['count'] / sum(df_fem_nms['count'])\n",
    "df_masc_nms['prob'] = df_masc_nms['count'] / sum(df_masc_nms['count'])\n",
    "df_surnames['prob'] = df_surnames['count'] / sum(df_surnames['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f7b6da6-121d-4c94-99da-36ed5237a08c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_full_names(n):\n",
    "    # generate male names\n",
    "    masc_firsts = np.random.choice(df_masc_nms.name, floor(n/2), p=df_masc_nms.prob)\n",
    "    # generate female names\n",
    "    fem_firsts = np.random.choice(df_fem_nms.name, ceil(n/2), p=df_fem_nms.prob)\n",
    "    # generate last names\n",
    "    surnames = np.random.choice(df_surnames.name, n, p=df_surnames.prob)\n",
    "    # zip results\n",
    "    list_nms = list(zip(np.append(masc_firsts, fem_firsts), surnames))\n",
    "    # concat first + last and return\n",
    "    return list(map(lambda x: x[0] + \" \" + x[1], list_nms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcb874df-15e2-479e-8272-b29ef7323f96",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['JOSEPH CERVANTES',\n",
       " 'RYAN FOX',\n",
       " 'STEVEN WILLIAMS',\n",
       " 'SAMUEL JOHNSON',\n",
       " 'EVERETT CLARK']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_full_names(100)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baba8b4-a73e-471d-b583-62c1cc6414ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NAME + ID GROUPINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780a4db9-5ac2-41df-b26e-c3cc4c104d77",
   "metadata": {},
   "source": [
    "Next, I add some duplication to the data to ensure that it works for groups (documents) with more than one name. I also add an ID to act as the primary key to group the different names into documents. The goal (and intent) of this project is to group these names with social security numbers (SSNs) as the primary key, but again to avoid any PII (or even appearance of using PII in any capacity), I opted into 1 through N numeric IDs instead. This serves the same purpose of providing a clear way to group the names into separate documents, while clearly not being real-world data.  \n",
    "\n",
    "The duplication is pretty basic; based on the percentage of duplicate names parameter, some percentage of the names will be duplicated once. There is a chance that duplicate names are generated from the mixture model as well, but for smaller corpuses this is fairly unlikely. After that, some percentage of the IDs are also duplicated. This is done by randomizing the existing IDs and duplicating some number of them based on percentage duplicate ID. This is done for 2 rounds, although more can be specified if wanted. Note that the maximum number of times an ID can be duplicated is 2^(rounds), and I decided that having between 1 to 4 duplicate IDs should be enough for my purposes.\n",
    "<br>\n",
    "<br>\n",
    "#### KEY ASSUMPTIONS\n",
    "- names with the same ID are not more likely to be similar. This does NOT reflect real world data, where misspellings, relations, changed first/last names are all much more likely to be a different name with the same ID (remember the ID is an SSN stand-in)\n",
    "- names are not duplicated based on their rate of occurance in the real world (although any naturally occuring duplicates would be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4a67773-d550-4daf-b865-fe3f11b33383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for some number of names, add a copy of it to the database\n",
    "# assign each a row number\n",
    "# for some number of names (iterations?), subtract 1 from id number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5b0e945-2ceb-48e4-a005-01a059c76882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_name_id_corpus(n, perc_duplicate_nm=.2, perc_duplicate_id=.1, rounds_duplicate_id=2):\n",
    "    # generate names\n",
    "    list_nms = generate_full_names(ceil(n * (1-perc_duplicate_nm)))  # make 1 - perc_duplicate_nm original names (still possible duplicates but rare)\n",
    "    list_nms += list_nms[:floor(n * (perc_duplicate_nm))]  # append copy perc_duplicate_nms from front of list to end of list\n",
    "    df = pd.DataFrame(list_nms, columns=['name'])\n",
    "\n",
    "    # generate ids\n",
    "    n_dupl = floor(n * perc_duplicate_id)  # get amount to duplicate per round\n",
    "    ids = list(df.index)  # gen initial list of ids\n",
    "\n",
    "    for _ in range(rounds_duplicate_id):\n",
    "        np.random.shuffle(ids)  # shuffle existing list\n",
    "        ids[:len(ids)-n_dupl-1:-1] = ids[:n_dupl]  # and make the last n_duplicate = first n_duplicate\n",
    "\n",
    "    df['id'] = ids  # assign duplicated ids to df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e330e5-a25c-4ea5-bed9-35ae92abb8ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "### CREATE CORPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30f0ff4-5885-4874-9ec2-f79de6562bf1",
   "metadata": {},
   "source": [
    "Actually create the corpus! A combination of generating the names, duplicating them, then generating, duplicating, and adding IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af5d72d-5087-4b30-8899-db680b10e4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_corpus = generate_name_id_corpus(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40ee538f-2c64-4358-a0f4-db99c3727893",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COOPER LOPEZ</td>\n",
       "      <td>8241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JACE WILLIAMS</td>\n",
       "      <td>6112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAMERON LYONS</td>\n",
       "      <td>3948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LANDON LONG</td>\n",
       "      <td>1544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KENNETH SILVA</td>\n",
       "      <td>5886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name    id\n",
       "0   COOPER LOPEZ  8241\n",
       "1  JACE WILLIAMS  6112\n",
       "2  CAMERON LYONS  3948\n",
       "3    LANDON LONG  1544\n",
       "4  KENNETH SILVA  5886"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09abe83f-2336-4604-b7bb-c3bbd80b38f2",
   "metadata": {},
   "source": [
    "### CREATE TEST DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f7057e-03e9-46d9-bdda-5fc2b6901fa4",
   "metadata": {},
   "source": [
    "I also have code here to generate test data, although I did not end up using it further in the process. If there was a need to do evaluation of the accuracy of document ranking or string similarity algorithms, this would serve as a way to provide names that were both form the original corpus and generated from the same model but not necessarily from the corpus to gauge the performance on both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dfb9171-d355-4c92-afd1-ac0d5d22092d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_name_id_test(n, df_corpus, perc_from_corpus=.5):\n",
    "    # sample from corpus for some perc, generate the rest using same function\n",
    "    n_new = floor(n * (1 - perc_from_corpus))\n",
    "    df_n = generate_name_id_corpus(n_new)  # new data\n",
    "    df_n['source'] = \"new\"\n",
    "\n",
    "    n_corpus = ceil(n * perc_from_corpus)\n",
    "    df_c = df_corpus[:n_corpus].copy()  # corpus data\n",
    "    df_c['source'] = \"corpus\"\n",
    "\n",
    "    return pd.concat([df_n, df_c]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "468a3e1b-b1a8-4102-87a3-117032ea91c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = generate_name_id_test(100, df_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af2d4881-8c1c-4892-8ae4-7400da40f5c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NATHAN HUDSON</td>\n",
       "      <td>42</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JONATHAN WEBER</td>\n",
       "      <td>12</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AVERY PEREZ</td>\n",
       "      <td>46</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BENJAMIN CRUZ</td>\n",
       "      <td>3</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANDREW BAKER</td>\n",
       "      <td>4</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name  id source\n",
       "0   NATHAN HUDSON  42    new\n",
       "1  JONATHAN WEBER  12    new\n",
       "2     AVERY PEREZ  46    new\n",
       "3   BENJAMIN CRUZ   3    new\n",
       "4    ANDREW BAKER   4    new"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf99d57f-cceb-4886-a0ae-93d519933b25",
   "metadata": {},
   "source": [
    "### ID -> TOKEN AND TOKEN -> ID CORPUS DICTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72fc7bc-76c0-47c5-9bc5-5d6bfc9f5e37",
   "metadata": {},
   "source": [
    "Here I create two indices in the form of dictionaries: a list of all tokens per each ID, and a list of all IDs for each token. The list of all IDs for each token provides a simple way to get a list of all documents that should be considered based on the query text, and the list of all tokens for each ID is key for calculating IDF, as well as for having a list of all tokens that exist in the corpus.  \n",
    "<br>\n",
    "#### KEY ASSUMPTIONS\n",
    "- this is inefficient at scale, iterating row-by-row is almost never the right way to do something like this. Larger-scale solutions would require a more distributed or database-centric approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5460ddd5-8c77-4fd2-9c64-fa10628c9b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_corpus['tokens'] = df_corpus.name.str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33e33226-e2d1-479c-9835-e34e2e355f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_token_dict = {}\n",
    "token_id_dict = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d0bae32-4e3a-4a82-bb25-a957e9c15129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, row in df_corpus.groupby('id')[['tokens']].sum().iterrows():\n",
    "    id_token_dict[i] = list()\n",
    "    for token in set(row.tokens):\n",
    "        id_token_dict[i].append(token)  # dict of every token related with a specific ID\n",
    "        token_id_dict[token].append(i)  # dict of every ID related with a specific token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a09b2-3339-4e3a-952b-38c30a4fbab6",
   "metadata": {},
   "source": [
    "### TF / IDF COUNTER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33299c12-958c-4082-86e6-bfe4a4af277c",
   "metadata": {},
   "source": [
    "Separate TF / IDF Counter objects or dictionaries are not required, since the needed values can be derived from the above dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "787204ed-4985-475c-9ad6-ad84253e1c3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TF = 1 (since working w/ sets)\n",
    "# IDF = 1 / len(token_id_dict[TOKEN])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171543d-5150-49b3-a866-ae552d273599",
   "metadata": {},
   "source": [
    "### SIMILAR QUERY TOKEN SUBSTITUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d8d36-b0de-4cb9-b371-bf9b2cafefc5",
   "metadata": {},
   "source": [
    "The first use of string similarity metrics in the process, here I create a function to account for misspellings or unknown tokens (although Jelinek-Mercer smoothing also accounts for non-existent tokens!). First, the query is broken into separate name tokens, and each is check against the list of all tokens in the corpus. If the token does exist, it is kept as is. If the token does NOT exist, then a string similarity function is used to compare the token from the query with all the tokens in the corpus, the most similar corpus token is chosen, and the missing query token is replaced. This serves as a basic spell check, as well as accounting for any names that may have rare or unusual spellings (not uncommon for names!).  \n",
    "\n",
    "For both this token substitution and the name similarity tiebreakers later I use the Jaro-Winkler simlarity score. Jaro-Winkler is a well regarded string matching algorithm, especially for names, although it still returns some unintuitive results (as any string similarity metric does!). The functions I wrote only expect a two-argument similarity function as an argument, rather than insisting on Jaro-Winkler, so there is room for further experimentation if wanted.  \n",
    "<br>\n",
    "#### KEY ASSUMPTIONS:\n",
    "- that the corpus will have some token that is similar to the misspelled or rare token in the query. Since it is generated, there is even a chance that names in the source CSVs don't actually appear in the corpus\n",
    "- that Jaro-Winkler is the best string similarity metric to use! In my opinion it would be the best for a real world dataset, but any similarity metric is going to have issues given that it is matching against a small corpus (at MOST ~800 unique tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "318bd421-a2e3-4a04-aee0-2274cc3b300b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def substitute_similar_tokens(orig_query, token_id_dict, similarity_function):\n",
    "    new_query = \"\"\n",
    "\n",
    "    for query_token in orig_query.split():  # for each otken in query...\n",
    "        token_comps = []\n",
    "        new_token = \"\"\n",
    "        if query_token not in token_id_dict:  # check if exists in corpus\n",
    "            for doc_token in token_id_dict.keys():  # if not...\n",
    "                token_comps.append((doc_token, similarity_function(query_token, doc_token)))  # get similarity score with all corpus tokens\n",
    "            new_token = sorted(token_comps, key=lambda x: x[1], reverse=True)[0][0]  # return corpus token with closest match\n",
    "            new_query += new_token\n",
    "        else:\n",
    "            new_query += query_token\n",
    "        new_query += \" \"\n",
    "\n",
    "    return new_query.strip()  # return new query, removing trailing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fc94e15-f035-4a9b-922c-cc5a4747e19e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JOHN SMITH'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substitute_similar_tokens(\"JOOHN SMIITH\", token_id_dict, JaroWinkler.similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd43c6b2-532d-4bb2-9f5d-53a5480d9512",
   "metadata": {},
   "source": [
    "### JELINEK-MERCER SMOOTHING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f9e3e-ed77-4d9e-b179-3e5f8fd08b08",
   "metadata": {},
   "source": [
    "Implementation of the Jelinek-Mercer (JM) Smoothing from the lecture, assuming that each token is from a set (so TF = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49087741-dee1-46f3-b781-198280128a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for each word in query, need...\n",
    "# count word in the document // token->id dict only returns ids w/ token\n",
    "# len document // len(id_token_dict[id])\n",
    "# count word in corpus // len(token_id_dict[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a267587-6f9d-4c5b-a3b3-c19d1831b1ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jm_smoothing(len_doc, prob_token_in_corp, lmb=.2):\n",
    "    return log(1 + (1-lmb) / (lmb * len_doc * prob_token_in_corp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fcd140-f28e-4315-a5d4-3467661582a4",
   "metadata": {},
   "source": [
    "### DOCUMENT RANKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dcb45b-3f80-463c-a649-187d5112d64e",
   "metadata": {},
   "source": [
    "Use the JM Smoothing algorithm to provide a score for each document based on the tokens in the query. For this search, a document is considered as all the names that share the same ID (SSN in a real world application), and the query is the provided name to be searched. A real world implementation could also apply similarity matching to the IDs as well, for example only including IDs in the document aggregation that have a Hamming distance of 2 or less from a query ID or IDs provided along with the query name.  \n",
    "  \n",
    "For this implementation, I take the top 10 results based on the JM Smoothing ranking and note the relevant scores. This is used in the next step for checking the string similarity between the query and the document names as a tie breaker and for further ranking of relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2984a940-ba23-4583-9772-1dd999197e70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_query_document_scores(query, token_id_dict, id_token_dict):\n",
    "    doc_scores = defaultdict(int)\n",
    "    # calculate rankings based on JM smoothing\n",
    "    for token in query.split():  # for each token in the query...\n",
    "        list_ids = token_id_dict[token]  # get the list of ids with that token\n",
    "        for doc_id in list_ids:  # for each document in that list...\n",
    "            jm_score = jm_smoothing(len(id_token_dict[doc_id]), len(token_id_dict[token])/len(id_token_dict))  # calculate the JM smoothing score for the matched token\n",
    "            doc_scores[doc_id] += jm_score  # add to documents overall score\n",
    "\n",
    "    return doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b79599d2-69e3-420e-9a5d-0b2206008c17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY Sent: JOHN SMITH\n",
      "\n",
      "3958 10.075826093753555\n",
      "6376 8.70780814555837\n",
      "348 5.9234046817938175\n",
      "748 5.9234046817938175\n",
      "1356 5.9234046817938175\n",
      "1689 5.9234046817938175\n",
      "2252 5.9234046817938175\n",
      "2509 5.9234046817938175\n",
      "2809 5.9234046817938175\n",
      "3123 5.9234046817938175\n"
     ]
    }
   ],
   "source": [
    "query = \"JOOHN SMIITH\"\n",
    "# uncomment if want query substitution\n",
    "query = substitute_similar_tokens(query, token_id_dict, JaroWinkler.similarity)\n",
    "\n",
    "# calculate rankings for all documents\n",
    "doc_scores = calculate_query_document_scores(query, token_id_dict, id_token_dict)\n",
    "\n",
    "tie_break_scores = set()\n",
    "# get the top 10 results in the initial JM score rankings\n",
    "print(f\"QUERY Sent: {query}\\n\")\n",
    "for doc_id in sorted(doc_scores, key=doc_scores.get, reverse=True)[:10]:\n",
    "    print(doc_id, doc_scores[doc_id])\n",
    "    tie_break_scores.add(doc_scores[doc_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52bb81-4e7b-44b0-a93e-c5e282b2277a",
   "metadata": {},
   "source": [
    "### DOCUMENT RANKING NAME SIMILARITY TIEBREAKERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ca78eb-604a-4b4a-87b3-f402a62c4bcc",
   "metadata": {},
   "source": [
    "The second use of string similarity metrics in the process, this serves as an additional ranking to be applied after the JM Smoothing ranking has been calculated and the top results returned. For each of the top 10 scores I note the score associated, and then all IDs of those scores are gathered and their ranking recalculated with a different metric. For my implementation I took the MAX of all the scores created by comparing the query to all the document names, but the AVG could be taken as well (although there are pros and cons to both). After a MAX similarity score has been calculated for each ID, the results are re-ranked and the final ranking is provided.  \n",
    "<br>\n",
    "#### KEY ASSUMPTIONS:\n",
    "- comparing the entire query to each document name with Jaro-Winkler is a good similarity measure\n",
    "- query names and document names are in the same order (\"JOHN SMITH\" <> \"SMITH JOHN\")\n",
    "- MAX similarity score over AVG similarity score\n",
    "- arbitrary ordering for tied similarity score\n",
    "- top 10 JM SMoothing results is a large enough window! For these specific examples, the assumptions called out in earlier portions show their effects... specifically, that a document is NOT more likely to have names that are similar to each other. As a result, this implementation is less likely to match to IDs with 2+ names for queries with only 2 tokens, since the resulting discount from document length in the JM Smoothing algorithm leads to them ranking lower. That said, queries with 3+ tokens will have a higher probability to show documents with 3+ tokens as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1081f42f-9f82-4167-bd89-a8d3fd4e55d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_query_document_tiebreaks(query, tie_break_ids, df_corpus, similarity_function):\n",
    "    doc_sim_scores = {}\n",
    "\n",
    "    for doc_id in tie_break_ids:  # for each document considered for tiebreaks...\n",
    "        max_sim_score = 0\n",
    "        for doc_name in df_corpus[df_corpus['id'] == doc_id]['name']:  # get each associated name\n",
    "            max_sim_score = max(max_sim_score, similarity_function(query, doc_name))  # get max similarity score between name and query\n",
    "        doc_sim_scores[doc_id] = max_sim_score\n",
    "\n",
    "    return doc_sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef777db7-fce8-4540-8a2c-7b5bd3ddf691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_doc_ids = defaultdict(list)\n",
    "tie_break_ids = []\n",
    "\n",
    "# create reverse dictionary of each id associated with certain final JM score (for tie breaking)\n",
    "for doc_id in doc_scores.keys():\n",
    "    score_doc_ids[doc_scores[doc_id]].append(doc_id)\n",
    "\n",
    "# get list of ids associated with tiebreak scores\n",
    "for score in tie_break_scores:\n",
    "    tie_break_ids += score_doc_ids[score]\n",
    "\n",
    "# calculate tie-break rankings for all documents with score from JM top 10\n",
    "doc_sim_scores = calculate_query_document_tiebreaks(query, tie_break_ids, df_corpus, JaroWinkler.similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b7d18f89-aa13-4901-ae5e-7f46c9f7f9e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df:                 name    id             tokens\n",
      "558       JOHN SMITH  6376      [JOHN, SMITH]\n",
      "9441  JORDAN SIMMONS  6376  [JORDAN, SIMMONS]\n",
      "avg_sim_score:1.0\n",
      "\n",
      "df:             name    id         tokens\n",
      "8558  JOHN SMITH  3958  [JOHN, SMITH]\n",
      "avg_sim_score:1.0\n",
      "\n",
      "df:             name   id         tokens\n",
      "8577  JOHN HICKS  348  [JOHN, HICKS]\n",
      "avg_sim_score:0.895\n",
      "\n",
      "df:             name    id         tokens\n",
      "8839  JOHN MILLS  4412  [JOHN, MILLS]\n",
      "avg_sim_score:0.895\n",
      "\n",
      "df:            name    id        tokens\n",
      "8258  JOHN REID  7989  [JOHN, REID]\n",
      "avg_sim_score:0.8533333333333333\n",
      "\n",
      "df:            name    id        tokens\n",
      "3931  JOHN TRAN  8029  [JOHN, TRAN]\n",
      "avg_sim_score:0.8533333333333333\n",
      "\n",
      "df:             name    id         tokens\n",
      "2413  JOHN ORTIZ  1356  [JOHN, ORTIZ]\n",
      "avg_sim_score:0.8514285714285714\n",
      "\n",
      "df:             name    id         tokens\n",
      "2886  JOHN DAVIS  2809  [JOHN, DAVIS]\n",
      "avg_sim_score:0.8514285714285714\n",
      "\n",
      "df:             name    id         tokens\n",
      "2723  JOHN RAMOS  4316  [JOHN, RAMOS]\n",
      "avg_sim_score:0.8514285714285714\n",
      "\n",
      "df:                 name   id             tokens\n",
      "2353  JOHN DOMINGUEZ  748  [JOHN, DOMINGUEZ]\n",
      "avg_sim_score:0.8400000000000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get top 10 results in tiebreak rankings\n",
    "for doc_id in sorted(doc_sim_scores, key=doc_sim_scores.get, reverse=True)[:10]:\n",
    "    print(f\"df: {df_corpus[df_corpus.id == doc_id]}\\nmax_sim_score:{doc_sim_scores[doc_id]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1a91cf-fcaa-4491-a003-1744cb8a0738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_custom_py3",
   "language": "python",
   "name": "conda_custom_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
